# Decision Log — Sports Analytics Intelligence Platform

> This is the **interview cheat sheet**. Every architectural and technical decision is documented here with alternatives considered, rationale, trade-offs, and interview angles.

---

## How to Read This Log

| Column | Purpose |
|--------|---------|
| **Decision** | What we chose |
| **Alternatives** | What else we could have used |
| **Why This Choice** | The reasoning — this is what you say in interviews |
| **Trade-off** | What we gave up — shows mature thinking |
| **Interview Angle** | How to frame this if asked |

---

## Infrastructure Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 1 | **FastAPI + HTML/CSS/JS** over Streamlit | Streamlit, Gradio, Dash | Premium visual impact, decoupled API+frontend architecture (production pattern), broader skill signal on resume | More development time than Streamlit | "I designed a decoupled ML system — API serves predictions, frontend consumes them. This is the pattern used in production ML systems." |
| 2 | **PostgreSQL** over SQLite | SQLite, MongoDB, DuckDB | Window functions for rolling features, JSONB for flexible metadata, concurrent access for API + data pipeline, production-grade RDBMS | Requires Docker setup (but trivial) | "I used PostgreSQL because I needed window functions for time-series feature computation and concurrent read/write for the API and ingestion pipeline." |
| 3 | **Docker Compose** for infrastructure | Local installs, Kubernetes | Reproducible environment, single-command setup, consistent across dev/prod | Learning curve for Docker beginners | "I containerized the stack for reproducibility — any team member can run `docker-compose up` and have the full environment." |
| 4 | **ChromaDB** over Pinecone | Pinecone, Weaviate, FAISS | Free, local, Python-native, no cloud costs or API limits | Less scalable than managed solutions | "I chose a local vector store to avoid cloud dependency and keep costs zero while maintaining full functionality." |
| 5 | **Gemini LLM** for RAG | OpenAI GPT, Claude, Llama local | Free tier available, good quality, user already has API key | Vendor dependency (but designed model-agnostic) | "I designed the RAG layer model-agnostic — swapping LLMs requires changing one config value." |

## ML Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 6 | **XGBoost + LightGBM** over Neural Nets | Deep Learning, Random Forest, SVM | Tabular data with <100 features — GBMs consistently outperform NNs on tabular data (Grinsztajn et al., 2022 NeurIPS benchmark) | Less flexible for unstructured/sequence data | "For structured tabular data, gradient boosted trees remain SOTA. The Grinsztajn benchmark showed NNs only win when data exceeds 10K+ features or is inherently sequential." |
| 7 | **SHAP** over LIME | LIME, Permutation Importance, Partial Dependence | Consistent Shapley values with mathematical guarantees, handles feature interactions, model-agnostic | Computationally slower than LIME | "SHAP provides theoretically grounded explanations based on cooperative game theory. Unlike LIME, SHAP values satisfy consistency, missingness, and local accuracy axioms." |
| 8 | **Kelly Criterion** over flat staking | Flat %, Proportional, Fixed amount | Mathematically optimal capital growth rate (Shannon, 1956), accounts for both edge AND odds | Can be aggressive — hence fractional Kelly | "I used the Kelly Criterion from information theory for capital allocation. It maximizes the geometric growth rate of capital. In practice, I use fractional Kelly (quarter/half) to reduce variance." |
| 9 | **Ensemble** (weighted avg) | Single best model, Stacking, Blending | Reduces variance without adding complexity, robust to individual model failures | Slightly higher latency | "Ensembling reduces variance — if one model overfits to a pattern, the ensemble smooths it out. I use validation-set performance to weight the models." |

## Data Pipeline Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------| 
| 10 | **nba_api** (Python package) over web scraping | BeautifulSoup scraping, paid APIs (SportsRadar), manual CSV downloads | Official NBA.com endpoints → structured JSON, no HTML parsing fragility, free, actively maintained by community | No real-time streaming (batch only), rate-limited to ~1 req/sec | "I used the official NBA stats API wrapper because it provides structured, reliable data without the fragility of web scraping. In production, API > scraping for maintainability." |
| 11 | **SQLAlchemy Core** over raw psycopg2 | Raw psycopg2, Django ORM, Peewee | Connection pooling out of the box, database-agnostic (can swap to MySQL/SQLite), Alembic integration for migrations, supports both raw SQL and ORM patterns | Slightly more abstraction than raw driver | "I used SQLAlchemy because it provides connection pooling — critical for handling concurrent FastAPI requests — and makes the data layer database-agnostic." |
| 12 | **Config-driven** (settings.yaml) over hardcoded constants | Hardcoded, JSON config, Python config module, env-vars only | YAML is human-readable, supports comments, hierarchical structure for nested config, separates config from code | Requires a YAML parser (trivial) | "I externalized all tuning parameters to YAML — model hyperparameters, risk thresholds, data sources. This means A/B testing different configs requires zero code changes, just a config swap." |
| 13 | **Separated raw data vs. computed features** (two tables) over single denormalized table | Single `enriched_games` table, feature columns alongside raw stats | Raw data is immutable — changing feature logic doesn't require re-ingesting from NBA API, clear lineage (raw → features → predictions), can recompute features without touching source data | More JOINs required | "I separated raw observations (`team_game_stats`) from computed features (`match_features`) following the immutable raw data principle. This means I can iterate on feature engineering without re-ingesting data — crucial for rapid experimentation." |
| 14 | **Documented Header-Patching Fallback** for `nba_api` | Upgrade runtime/library first, Proxy/VPN, Switching to paid API | The production baseline remains direct `nba_api` usage plus resilient retry/rate-limit controls. Header patching is documented as an operational fallback when upstream behavior changes and immediate runtime upgrades are not feasible. | Fallback patches must be documented and version-pinned to avoid drift. | "I treat header engineering as a contingency playbook. The default path is stable library usage with observability, and only when upstream blocks occur do we apply targeted request-header patches." |
| 15 | **Incremental DB Sync & Watermarking** | Truncate and load daily, full season pull | Selecting the `MAX(game_date)` from the DB to filter API responses prevents redundant upserts, reducing a full season pull (~1200 games) to a tiny delta (~5-10 games) in daily runs. | Must handle the edge case where no data exists (initial load) | "Instead of a naive 'truncate and reload' or repeating heavy upserts daily, I implemented watermarking: selecting the max date from the database and fetching only games on or after that date. This optimized our API rate limits and lowered database IO." |
| 16 | **In-Database Feature Engineering (SQL)** | Python/Pandas processing | SQL Window Functions are optimized for time-series (rolling stats) and relation ops (self-joins for H2H). Pushing computation to the data layer eliminates data movement overhead (moving 100k rows to Python) and ensures consistency across training/serving. | Complex SQL is harder to unit test than Python functions | "I pushed feature engineering into the data layer using PostgreSQL window functions. This is the 'Senior Manager' approach to performance: push computation to the data to avoid the 'Big Data' tax of moving millions of rows into Python memory." |
| 17 | **Resilient API Handler (Exp. Backoff)** | Simple try/except, single delay | NBA.com is flaky; 502/504/429 errors are common. A robust decorator with jittered exponential backoff ensures the pipeline completes even if the network or API flickers. | Longer execution time for failed runs | "I designed the ingestion layer to be 'safe to fail' by implementing an exponential backoff decorator. If the NBA API throttles us, the system waits progressively longer before retrying, ensuring 99.9% pipeline reliability without manual intervention." |
| 18 | **Centralized `config.py` Implementation** | Dotenv alone, Hardcoded constants, Environment variables only | Combining `python-dotenv` with a central `config.py` allows for type-safety, default values, and a single file to update when the tracking season changes. | Slightly more boilerplate, but prevents "constant drift" across multiple modules. | Highlighting "Single Source of Truth" architectural principles. |
| 19 | **Automated Post-Ingestion Audit** | Unit tests on dataframes, Manual spot checks | SQL-based set logic is much faster for checking cross-table consistency (e.g., verifying 2 team records per match). | Adds ~1-2 seconds to the ingestion run, but saves hours of debugging "missing data" in ML training. | "Data Quality as a First-Class Citizen." |
| 20 | **Numeric Data Integrity (`DECIMAL` over `FLOAT`)** | `FLOAT`, `REAL` | Floating point errors in Python/DB aggregations can lead to precision loss (e.g., 0.1+0.2 != 0.3). `DECIMAL(5,3)` ensures exact precision for ML inputs like Shooting % or Offensive Rating. | Slightly more storage/memory | "In betting models, where 0.5% edge is everything, I used exact numeric types (`DECIMAL`) to avoid floating point math errors that could silently bias our model inputs." |
| 21 | **Jittered Rate Limiting** | Fixed delay, No delay | Fixed intervals are easily flagged by anti-scraping firewalls. Random jitter mimics organic human-like behavior. | Slightly unpredictable execution time | "Scraping detection often looks for 'robotic' periodicity. I added randomized jitter to our rate limiting to make the pipeline's footprint indistinguishable from human browsing patterns." |
| 22 | **Dual-Handler Pipeline Logging** | Console-only, CloudWatch/External | Logging to both console (for dev) and a persistent log file (for production traceability) ensures we have a post-mortem trail without needing an external service. | Local disk usage (requires management/rotation) | "I implemented dual-layer logging — real-time visibility in the console and persistent traceability in file logs. This ensures zero data loss for operational tracking." |
| 23 | **Audit-First Ingestion Workflow** | Python-based logs only, No audit | Recording every sync event (record counts, status, errors) in a structured DB table allows for the construction of health dashboards and proactive monitoring. | Extra DB table and write overhead | "I treated observability as a core feature by building a structured auditing layer. Every sync records its 'Pulse' in the database, enabling me to build a health dashboard that proves data reliability before any ML model starts training." |
| 24 | **Structured Audit Summary Contract** | Log-only warnings, ad-hoc SQL checks | Returning machine-readable audit results (`team_stats_violations`, `player_stats_missing_games`, `null_score_matches`, `passed`) makes quality status easy to persist, query, and surface to clients. | Slightly more schema discipline needed in code/tests | "I moved data-quality checks from human-only logs to a formal contract so observability can drive product behavior." |
| 25 | **Additive System Status Exposure** (`pipeline.audit_violations`) | New endpoint, breaking response change | Extending existing `/api/v1/system/status` payload avoids client breakage while adding useful quality telemetry. | Slightly larger response payload | "I shipped quality telemetry as an additive API field so existing consumers stayed stable." |
| 26 | **Rotating File Logs** over plain file handler | Unlimited file logs, external logging stack | Rotating handlers cap log growth and preserve recent history without requiring immediate external logging infrastructure. | Requires tuning rotation size/count | "I added log rotation to keep local ops simple while preventing disk bloat." |
| 27 | **Behavior-Aligned Route Tests** | Force app behavior to old tests, skip failing tests | Tests were updated to reflect actual runtime behavior (`/` serves static dashboard HTML, `/api/v1/health` returns `healthy`). | Requires clear test/docs synchronization | "I treat tests as executable product contracts; when behavior is intentional, tests must assert the real contract." |
| 28 | **Targeted Regression Suite Gate** | Broad but flaky test gate, no targeted gate | Running focused regression set (`ingestion_retry`, `routes`, `config`) gives fast confidence for Phase 0 hardening changes. | Not full end-to-end coverage alone | "I use layered test gates: fast targeted regression first, then deeper integration in follow-up." |
| 29 | **Advanced Metrics Backfill in Incremental Sync** | Full historical reload, One-off migration script, Leave nulls and impute in model | We added selective backfill detection for games missing advanced metrics (`off/def rating`, `pace`, `eFG`, `TS`) and include those game_ids during incremental pulls. This keeps daily sync cheap while repairing legacy incomplete rows. | Slightly more complex filter logic in ingestion | "I designed a targeted backfill strategy: keep watermark-based incremental ingestion, but include historical rows only when critical feature columns are missing. It gives migration quality without full reload cost." |
| 30 | **Two-Stage Defensive Rating Derivation** | Depend only on API-provided defensive metrics, Drop defensive rating feature, Build a separate ETL job | First attempt defensive rating from same-row inputs (`plus_minus` + estimated possessions). Then run a SQL backfill pass using opponent points from self-join when `plus_minus` is unavailable. This maximizes completeness while preserving idempotency. | Derived metric has approximation assumptions tied to possessions formula | "I implemented a fallback hierarchy for feature completeness. If one source field is absent, we derive defensive rating from opponent points and possessions in-DB. This is a production resilience pattern: graceful degradation, not silent nulls." |
| 31 | **Self-Healing Audit Table Bootstrap** | Require manual SQL fix, Reset Docker volume, Hard-fail pipeline on missing table | Legacy Docker volumes can drift from latest schema. We detect missing `pipeline_audit`, auto-create table/indexes idempotently, and retry audit insert once. `/system/status` also degrades gracefully to empty history if table is absent. | Runtime DDL in app code adds bootstrap responsibility to service layer | "I added a self-healing observability bootstrap. Instead of breaking ingestion on schema drift, the system repairs the missing audit table and continues. This reduces operational toil while keeping reliability visible." |
| 32 | **Prediction Operations API Split** (`/predictions/today` + `/predictions/performance`) | Keep only single-game prediction endpoint, Offline notebook-only evaluation | Separating batch serving from performance analytics creates a production-style contract: one route for daily inference, another for historical model governance. | More endpoints to maintain and test | "I separated online inference from evaluation endpoints so runtime serving and model governance can evolve independently." |
| 33 | **Idempotent Prediction Persistence + Outcome Sync** | Recompute predictions on demand only, Overwrite full tables daily | Persisting per-model predictions with `(game_id, model_name)` upsert and syncing `was_correct` post-game creates reproducible accuracy/calibration metrics. | Requires stateful storage lifecycle and backfill logic | "I designed prediction tracking as an auditable ledger, not an ephemeral response. That enables trustworthy backtesting and monitoring." |
| 34 | **Bankroll Ledger APIs over Spreadsheet Tracking** | Manual Excel tracking, Frontend-only local storage | A DB-backed `bets` ledger with create/list/settle/summary endpoints makes bankroll analysis queryable, testable, and automation-ready for risk controls. | Requires settlement rules and API contracts | "I treated bankroll operations as first-class data products. Once bets are in a ledger, ROI and risk metrics become deterministic system outputs, not manual calculations." |
| 35 | **Single-Page Operations Console (Module Layout)** | Multi-page navigation, separate micro-frontends | A single page with module cards/table sections reduces click-friction for daily operator workflows and aligns with interview demos where end-to-end flow must be shown quickly. | Larger page-level script and tighter coupling of module refresh behavior | "I optimized for operational visibility: one screen shows system health, model output, explainability, and bankroll impact." |
| 36 | **Vanilla Frontend Integration over Framework Migration** | React/Next.js rewrite, chart-heavy SPA from day one | At current scope, framework-free integration keeps delivery fast and deployability simple while still proving API-first architecture and UX discipline. | Manual state/render handling is more verbose than component frameworks | "I deferred framework complexity until product needs justified it. This is a pragmatic sequencing decision: prove value and contracts first, then scale the frontend stack." |
| 37 | **Tab-Based Information Architecture** (Home/Raw/Quality/Analysis) | Single long scroll page, multi-route app | Splitting workflows by operator intent makes the dashboard easier to reason about and keeps context-specific controls together (raw exploration vs. monitoring vs. analysis). | Slightly more navigation/state handling in JS | "I used intent-based tabs so each screen answers one question clearly: what data exists, is pipeline healthy, and what do models suggest?" |
| 38 | **Raw-Only Data Explorer Contract** (exclude `match_features`) | Expose all tables, hide DB exploration entirely | Restricting explorer APIs to raw tables keeps experimentation data separate and avoids accidental coupling between exploratory UI and model-feature internals. | Requires dedicated backend endpoints and whitelist maintenance | "I enforced a clean boundary: raw source truth is visible to operators, while engineered features remain controlled for modeling workflows." |
| 39 | **Graceful-Degradation RAG Runtime** (Gemini/Chroma primary, deterministic fallback) | Hard-fail when provider unavailable, mock data, provider lock-in | Local and interview environments are often dependency- or network-constrained. A fallback path preserves endpoint reliability and demonstrates resilient engineering. | Fallback summaries are less expressive than live LLM output | "I designed the intelligence layer to degrade safely: when LLM/vector infra is unavailable, the API still returns deterministic, audit-friendly outputs instead of 500 errors." |
| 40 | **Citation-Gated Intelligence Outputs** | Allow uncited summaries, confidence-only display | Enforcing citation presence reduces hallucination risk and keeps context explainable. This aligns with production governance expectations for LLM usage. | In sparse-context periods, output becomes conservative (`insufficient`) | "I implemented a citation gate: no sources means no authoritative narrative. That protects trust and keeps AI output reviewable." |
| 41 | **Deterministic Rule Overlay separate from model probability** | Blend rules directly into predicted probability, manual analyst override only | Keeping rule signals separate preserves statistical integrity of model outputs while still adding operational context. | Operators must read two signals (probability + context risk) | "I separated model inference from contextual governance. This avoids hidden probability mutations and keeps decision logic auditable." |
| 42 | **API-First MLOps Controls** (`/mlops/monitoring`, `/mlops/retrain/policy`) | Notebook-only monitoring, cron logs without API, manual retrain notes | API contracts make monitoring and retrain decisions portable to dashboards, automation, and alerts. | Additional maintenance surface for endpoints | "I productized MLOps signals as APIs, not notebook artifacts, so operations can consume them consistently across UI and automation layers." |
| 43 | **Model Artifact Snapshot in System Status** | Hidden filesystem state, separate admin-only endpoint | Exposing model artifact metadata in status payload improves operational transparency without extra tooling. | Slightly larger status payload | "I surfaced model artifact metadata in system status so teams can immediately verify which artifacts are active during incident/debug workflows." |
| 44 | **Heuristic Source Quality Scoring + Feed Health Telemetry** | Blind top-k retrieval, manual source review only, hard source whitelisting | Context quality varies by source/article type. Scoring freshness + similarity and penalizing noisy betting-only content provides a practical guardrail while preserving deterministic behavior. Feed-health telemetry exposes ingestion reliability to operators. | Heuristic tuning requires periodic recalibration and can underweight niche but useful content | "I added an explicit quality layer on top of retrieval so analysts can see not only *what* context was retrieved, but *how trustworthy/relevant* it is, with source health visible for operations." |
| 45 | **Noise-Aware Rule Evaluation for Injury Signals** | Apply injury keyword rules on all retrieved docs, fully manual override | Betting/promotional context can contain injury keywords that are irrelevant for operational game context. Ignoring noisy docs in rule evaluation reduces false positives while conflict signals preserve uncertainty when mixed evidence exists. | Some true signals in noisy sources may be skipped unless source-quality tuning improves | "I treated rule quality as a precision problem: first filter for signal quality, then run deterministic rules, and explicitly surface conflicts instead of hiding them." |
| 46 | **Persist Monitoring Snapshots + Expose Trend Endpoint** | Only latest metrics endpoint, external BI-only trends, log scraping | A dedicated snapshot table with a simple trend API keeps monitoring history queryable and immediately consumable by frontend/automation without extra tooling. | More storage and schema lifecycle responsibility | "I moved MLOps monitoring from point-in-time health checks to a time-series contract so drift and alert patterns can be observed, not inferred." |
| 47 | **Deterministic Escalation Policy for MLOps Alerts** | Ad-hoc manual escalation, severity-only alerts, alert fatigue without action guidance | Alert payloads now include breach streaks and recommended actions, while top-level escalation state drives incident workflow. This makes on-call behavior deterministic and auditable. | Requires threshold tuning to avoid over/under-escalation | "I converted monitoring from passive dashboards to action-oriented operations by encoding escalation decisions directly in the API contract." |
| 48 | **Queue-First Retrain Automation with Duplicate Guard** | Immediate retrain execution on trigger, manual queue tracking, cron-only scripts | Queueing retrain jobs through policy execute-mode (`dry_run=false`) preserves control, auditability, and rollback planning before full worker automation is introduced. Duplicate guard prevents runaway job spam. | Adds queue lifecycle management overhead and requires worker integration for full automation | "I deliberately implemented retrain automation as a controlled queue first. It gives us deterministic policy execution and audit logs without risking uncontrolled retrain loops." |
| 49 | **Safe Worker Execution Modes for Retrain Jobs** | Only full execution mode, manual DB status updates, no simulation path | Supporting `execute=false` for worker runs enables operational validation of lifecycle transitions before launching expensive/full training runs. This reduces production risk during rollout. | Requires operators to explicitly choose execution mode | "I added a safety-first rollout pattern: the same worker can run in simulation or execute mode, so lifecycle and observability can be validated before compute-heavy retraining." |
| 50 | **Targeted GitHub Actions Regression Gate** | Local-only testing, full test suite on every push, no CI checks | A focused CI workflow (`test_ingestion_retry`, `test_routes`, `test_config`) provides fast, deterministic guardrails on every push/PR to `main` while keeping runtime short. | Does not replace full integration/performance testing | "I introduced layered quality gates: a fast CI regression contract catches critical breaks early, while deeper suites remain available for release validation." |
| 51 | **DB-Backed Invariant Tests with Temp-Table Isolation** | Mock-only invariant tests, shared persistent test schema, no negative-path fixtures | Running invariant checks against a real PostgreSQL engine validates true SQL behavior while temp-table shadowing keeps tests isolated and deterministic. | Requires local DB availability for full execution | "I used real DB integration tests for critical data-quality invariants, but isolated each case with temporary tables so production data remains untouched and tests stay repeatable." |
| 52 | **Runtime Compatibility Pin for LibreSSL Environments** | Force immediate local OpenSSL rebuild, ignore warning, pin full stack versions aggressively | Pinning `urllib3<2` removes noisy SSL-runtime warnings in local environments still linked to LibreSSL while CI remains on Python 3.11/OpenSSL baseline. | Delays migration to urllib3 v2 features until local runtime parity is complete | "I treated environment parity as an operations risk: stabilize local reliability first with a compatibility pin, then migrate once interpreter SSL stack is standardized." |

## UI Redesign & Chatbot Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 53 | **`ui-redesign` feature branch** with React + Vite + TypeScript (`frontend-v2/`) over in-place rewrite of the existing vanilla JS frontend | Refactor `frontend/` in-place, next.js migration, abandon old frontend | Keeps the production `main` dashboard fully operational while the new design matures in parallel. Feature branches are the industry standard for non-trivial UI migrations. | Two frontend directories exist until `ui-redesign` is merged — requires discipline to avoid cross-branch PRs | "I used a parallel branch strategy for the UI migration. It's the same pattern used for 'strangler fig' migrations in production systems — new and old coexist until the new is ready to take over." |
| 54 | **`chatbot` sub-branch off `ui-redesign`** rather than branching from `main` | Branch from `main` and cherry-pick UI, open separate PR to main, build chatbot directly on `ui-redesign` | Chatbot UI must be consistent with the new design system in `ui-redesign`. Branching from `ui-redesign` means all design tokens, components, and layout logic are inherited without cherry-picking. Merge flow: `chatbot → ui-redesign → main`. | Additional merge hop — chatbot changes need two PRs before reaching `main` | "I used a git branch hierarchy that mirrors the product dependency: chatbot depends on the new design system, so it should branch from and merge into the design branch, not bypass it. This keeps the merge graph clean and reviewable." |
| 55 | **Three-file component decomposition** for chatbot (`ChatMessage`, `ChatbotPanel`, `useChatbot`) | Single monolithic `Chatbot.tsx` page, inline state management | `useChatbot` hook is independently testable without rendering. `ChatMessage` is reusable (future: floating widget, notification feed). `ChatbotPanel` can be embedded anywhere in the layout (sidebar, modal, full-page). This is the React separation-of-concerns principle applied. | More files to navigate — justified by reusability and testability | "I separated presentation, composition, and state into distinct units. The hook is pure data logic, the message component is pure rendering, and the panel wires them together. Each layer can change without the others." |
| 56 | **Custom `useChatbot` hook** over React Context or Zustand for chat state | Global state (Zustand/Redux), React Context, component-local `useState` | Chat state (message history, loading flag, abort controller) is scoped to the Chatbot page — no other component needs it. Global state would be over-engineering. The hook encapsulates the AbortController lifecycle, preventing memory leaks on unmount automatically. | If chat needs to persist across navigation (future), promotion to Context/Zustand would be needed | "I followed the principle of minimal state scope: local state for local concerns. The hook pattern keeps side-effect cleanup (AbortController) co-located with the state it protects." |
| 57 | **Graceful degradation before `/api/v1/chat` backend is ready** (informative error message instead of broken UI) | Block UI until backend ready, show spinner indefinitely, hide chatbot behind feature flag | The UI is fully functional at the component level. When the backend returns a non-200, the user receives a contextual message explaining the integration status — not a broken spinner. This is the same pattern used in Decision #39 (RAG graceful degradation). | Users see an "in progress" message which may reduce confidence — mitigated by the informative copy | "I applied the graceful degradation pattern consistently: the frontend is never in an unknown broken state. An informative error is always better than a hanging spinner. This is production-grade error UX." |
| 58 | **Amber (`#F59E0B`) accent for Chatbot**, consistent with existing design token (`--accent-chat`) | New custom color, match another section's accent, monochrome | The design system already reserved amber for chatbot (`--accent-chat`) before the feature was built — a signal of good upfront design token planning. Reusing it ensures visual consistency across Navbar, Overview cards, and the Chatbot page itself. | Amber is also used for `--warning` — careful not to conflate warning states with chatbot branding | "I reused the pre-reserved design token rather than inventing a new color. This shows respect for the design system contract — components should consume tokens, not create new ones." |

## Chatbot Backend Integration Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 65 | **Hybrid intent routing** (RAG path + DB path) rather than a single LLM approach | Single LLM with full DB access, pure RAG only, pure SQL only | Each path is optimized for its domain: RAG excels at unstructured news/context retrieval; NL→SQL excels at structured analytics. A single LLM call with both context and schema would produce weaker answers and higher latency. The hybrid approach mirrors production systems (e.g. Slack's AI, Notion AI) that route to different backends by query type. | Routing mistakes can send a question down the wrong path — mitigated by scoring both keyword sets and defaulting to DB (the richer data source) | "I designed the chatbot with intent-aware routing. Stats questions go to SQL, news questions go to RAG. This is architecturally identical to how production AI assistants route to different knowledge sources — each tool is used for what it's good at." |
| 66 | **Keyword-based IntentRouter** (no LLM call) rather than LLM classification | LLM-based intent classification, embedding similarity to intent examples | LLM classification adds ~500-1000ms latency per message. For a chatbot where response speed matters, spending a full LLM round-trip just to classify would hurt UX. Keyword scoring is deterministic, instant (<1ms), and handles 95% of real queries correctly. | Cannot handle nuanced ambiguous queries as well as LLM classification — acceptable at current scale | "I applied the principle of using the lightest tool that satisfies the requirement. LLM classification is overkill for intent routing when keyword heuristics handle the common cases. The LLM is reserved for what only it can do: synthesizing answers." |
| 67 | **LLMClient abstraction class** as the single point of LLM contact | Inline Gemini calls throughout the service, provider-specific SDK calls scattered | The user's stated intent is to switch from Gemini to Minimax once it's live. Abstracting the LLM behind a single class means swapping providers requires changing exactly one file (`chat_service.py`, the `LLMClient` class), zero UI changes, zero route changes. This is the Adapter pattern from design patterns. | Adds one layer of indirection — trivial cost vs the migration flexibility it provides | "I applied the Adapter design pattern for the LLM integration. The rest of the system depends on the `LLMClient` interface, not on Gemini. When Minimax goes live, it's a one-class swap — this is how production AI platforms manage provider migrations." |
| 68 | **Dynamic schema fetching from `information_schema`** for NL→SQL prompting | Hardcoded schema string, no schema in prompt, one-time schema snapshot | Providing the actual live column names from `information_schema.columns` to the LLM dramatically reduces SQL hallucination. Hardcoded schemas drift as the DB evolves. Dynamic fetching is lazy-loaded and cached per request — adds ~5ms once, then reused within the conversation. | Adds one DB query per session — negligible cost | "For NL→SQL, I gave the LLM the actual database schema rather than a static string. This prevents column hallucination and automatically stays current as the schema evolves. It's the difference between giving someone a real map vs. a rough sketch." |
| 69 | **Off-topic gate before any LLM call** | Let LLM decide if question is relevant, no gate at all | Off-topic questions (recipes, politics, etc.) should never reach the LLM — not for cost reasons but for product integrity. The chatbot must stay focused on sports analytics. The keyword gate fires in microseconds and handles this at zero LLM cost. The gate also makes the product behavior predictable and auditable. | May over-block edge cases — acceptable since the product is narrowly scoped to sports | "I added a hard gate before any LLM call: if the question isn't about sports analytics, we decline immediately without touching the LLM. This keeps the product focused, costs minimal, and behavior auditable — you can read the reject rules directly." |
| 70 | **sport parameter in ChatService** for future multi-sport support | Hardcode 'nba', wait until multi-sport is needed to refactor | The user explicitly stated multi-sport support is planned. Adding `sport='nba'` as a parameter now costs nothing and prevents a future refactor from rippling through the frontend, routes, and service simultaneously. The `_SQL_SAFE_TABLES` and intent keywords can be extended per sport. The public API (`POST /api/v1/chat`) already accepts a `sport` field. | Slight over-engineering for current state — intentional technical investment | "I designed the service with a sport parameter from day one, even though only NBA is live. When cricket or football is added, the frontend passes `sport='cricket'`, the service loads sport-specific tables and keywords, and the rest of the stack is untouched. This is the Open-Closed principle applied to product extensibility." |

---


*This log is updated as new decisions are made throughout the project.*
