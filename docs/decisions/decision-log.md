# Decision Log — Sports Analytics Intelligence Platform

> This is the **interview cheat sheet**. Every architectural and technical decision is documented here with alternatives considered, rationale, trade-offs, and interview angles.

---

## How to Read This Log

| Column | Purpose |
|--------|---------|
| **Decision** | What we chose |
| **Alternatives** | What else we could have used |
| **Why This Choice** | The reasoning — this is what you say in interviews |
| **Trade-off** | What we gave up — shows mature thinking |
| **Interview Angle** | How to frame this if asked |

---

## Infrastructure Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 1 | **FastAPI + HTML/CSS/JS** over Streamlit | Streamlit, Gradio, Dash | Premium visual impact, decoupled API+frontend architecture (production pattern), broader skill signal on resume | More development time than Streamlit | "I designed a decoupled ML system — API serves predictions, frontend consumes them. This is the pattern used in production ML systems." |
| 2 | **PostgreSQL** over SQLite | SQLite, MongoDB, DuckDB | Window functions for rolling features, JSONB for flexible metadata, concurrent access for API + data pipeline, production-grade RDBMS | Requires Docker setup (but trivial) | "I used PostgreSQL because I needed window functions for time-series feature computation and concurrent read/write for the API and ingestion pipeline." |
| 3 | **Docker Compose** for infrastructure | Local installs, Kubernetes | Reproducible environment, single-command setup, consistent across dev/prod | Learning curve for Docker beginners | "I containerized the stack for reproducibility — any team member can run `docker-compose up` and have the full environment." |
| 4 | **ChromaDB** over Pinecone | Pinecone, Weaviate, FAISS | Free, local, Python-native, no cloud costs or API limits | Less scalable than managed solutions | "I chose a local vector store to avoid cloud dependency and keep costs zero while maintaining full functionality." |
| 5 | **Gemini LLM** for RAG | OpenAI GPT, Claude, Llama local | Free tier available, good quality, user already has API key | Vendor dependency (but designed model-agnostic) | "I designed the RAG layer model-agnostic — swapping LLMs requires changing one config value." |

## ML Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------|
| 6 | **XGBoost + LightGBM** over Neural Nets | Deep Learning, Random Forest, SVM | Tabular data with <100 features — GBMs consistently outperform NNs on tabular data (Grinsztajn et al., 2022 NeurIPS benchmark) | Less flexible for unstructured/sequence data | "For structured tabular data, gradient boosted trees remain SOTA. The Grinsztajn benchmark showed NNs only win when data exceeds 10K+ features or is inherently sequential." |
| 7 | **SHAP** over LIME | LIME, Permutation Importance, Partial Dependence | Consistent Shapley values with mathematical guarantees, handles feature interactions, model-agnostic | Computationally slower than LIME | "SHAP provides theoretically grounded explanations based on cooperative game theory. Unlike LIME, SHAP values satisfy consistency, missingness, and local accuracy axioms." |
| 8 | **Kelly Criterion** over flat staking | Flat %, Proportional, Fixed amount | Mathematically optimal capital growth rate (Shannon, 1956), accounts for both edge AND odds | Can be aggressive — hence fractional Kelly | "I used the Kelly Criterion from information theory for capital allocation. It maximizes the geometric growth rate of capital. In practice, I use fractional Kelly (quarter/half) to reduce variance." |
| 9 | **Ensemble** (weighted avg) | Single best model, Stacking, Blending | Reduces variance without adding complexity, robust to individual model failures | Slightly higher latency | "Ensembling reduces variance — if one model overfits to a pattern, the ensemble smooths it out. I use validation-set performance to weight the models." |

## Data Pipeline Decisions

| # | Decision | Alternatives | Why This Choice | Trade-off | Interview Angle |
|---|----------|-------------|-----------------|-----------|-----------------| 
| 10 | **nba_api** (Python package) over web scraping | BeautifulSoup scraping, paid APIs (SportsRadar), manual CSV downloads | Official NBA.com endpoints → structured JSON, no HTML parsing fragility, free, actively maintained by community | No real-time streaming (batch only), rate-limited to ~1 req/sec | "I used the official NBA stats API wrapper because it provides structured, reliable data without the fragility of web scraping. In production, API > scraping for maintainability." |
| 11 | **SQLAlchemy Core** over raw psycopg2 | Raw psycopg2, Django ORM, Peewee | Connection pooling out of the box, database-agnostic (can swap to MySQL/SQLite), Alembic integration for migrations, supports both raw SQL and ORM patterns | Slightly more abstraction than raw driver | "I used SQLAlchemy because it provides connection pooling — critical for handling concurrent FastAPI requests — and makes the data layer database-agnostic." |
| 12 | **Config-driven** (settings.yaml) over hardcoded constants | Hardcoded, JSON config, Python config module, env-vars only | YAML is human-readable, supports comments, hierarchical structure for nested config, separates config from code | Requires a YAML parser (trivial) | "I externalized all tuning parameters to YAML — model hyperparameters, risk thresholds, data sources. This means A/B testing different configs requires zero code changes, just a config swap." |
| 13 | **Separated raw data vs. computed features** (two tables) over single denormalized table | Single `enriched_games` table, feature columns alongside raw stats | Raw data is immutable — changing feature logic doesn't require re-ingesting from NBA API, clear lineage (raw → features → predictions), can recompute features without touching source data | More JOINs required | "I separated raw observations (`team_game_stats`) from computed features (`match_features`) following the immutable raw data principle. This means I can iterate on feature engineering without re-ingesting data — crucial for rapid experimentation." |
| 14 | **Documented Header-Patching Fallback** for `nba_api` | Upgrade runtime/library first, Proxy/VPN, Switching to paid API | The production baseline remains direct `nba_api` usage plus resilient retry/rate-limit controls. Header patching is documented as an operational fallback when upstream behavior changes and immediate runtime upgrades are not feasible. | Fallback patches must be documented and version-pinned to avoid drift. | "I treat header engineering as a contingency playbook. The default path is stable library usage with observability, and only when upstream blocks occur do we apply targeted request-header patches." |
| 15 | **Incremental DB Sync & Watermarking** | Truncate and load daily, full season pull | Selecting the `MAX(game_date)` from the DB to filter API responses prevents redundant upserts, reducing a full season pull (~1200 games) to a tiny delta (~5-10 games) in daily runs. | Must handle the edge case where no data exists (initial load) | "Instead of a naive 'truncate and reload' or repeating heavy upserts daily, I implemented watermarking: selecting the max date from the database and fetching only games on or after that date. This optimized our API rate limits and lowered database IO." |
| 16 | **In-Database Feature Engineering (SQL)** | Python/Pandas processing | SQL Window Functions are optimized for time-series (rolling stats) and relation ops (self-joins for H2H). Pushing computation to the data layer eliminates data movement overhead (moving 100k rows to Python) and ensures consistency across training/serving. | Complex SQL is harder to unit test than Python functions | "I pushed feature engineering into the data layer using PostgreSQL window functions. This is the 'Senior Manager' approach to performance: push computation to the data to avoid the 'Big Data' tax of moving millions of rows into Python memory." |
| 17 | **Resilient API Handler (Exp. Backoff)** | Simple try/except, single delay | NBA.com is flaky; 502/504/429 errors are common. A robust decorator with jittered exponential backoff ensures the pipeline completes even if the network or API flickers. | Longer execution time for failed runs | "I designed the ingestion layer to be 'safe to fail' by implementing an exponential backoff decorator. If the NBA API throttles us, the system waits progressively longer before retrying, ensuring 99.9% pipeline reliability without manual intervention." |
| 18 | **Centralized `config.py` Implementation** | Dotenv alone, Hardcoded constants, Environment variables only | Combining `python-dotenv` with a central `config.py` allows for type-safety, default values, and a single file to update when the tracking season changes. | Slightly more boilerplate, but prevents "constant drift" across multiple modules. | Highlighting "Single Source of Truth" architectural principles. |
| 19 | **Automated Post-Ingestion Audit** | Unit tests on dataframes, Manual spot checks | SQL-based set logic is much faster for checking cross-table consistency (e.g., verifying 2 team records per match). | Adds ~1-2 seconds to the ingestion run, but saves hours of debugging "missing data" in ML training. | "Data Quality as a First-Class Citizen." |
| 20 | **Numeric Data Integrity (`DECIMAL` over `FLOAT`)** | `FLOAT`, `REAL` | Floating point errors in Python/DB aggregations can lead to precision loss (e.g., 0.1+0.2 != 0.3). `DECIMAL(5,3)` ensures exact precision for ML inputs like Shooting % or Offensive Rating. | Slightly more storage/memory | "In betting models, where 0.5% edge is everything, I used exact numeric types (`DECIMAL`) to avoid floating point math errors that could silently bias our model inputs." |
| 21 | **Jittered Rate Limiting** | Fixed delay, No delay | Fixed intervals are easily flagged by anti-scraping firewalls. Random jitter mimics organic human-like behavior. | Slightly unpredictable execution time | "Scraping detection often looks for 'robotic' periodicity. I added randomized jitter to our rate limiting to make the pipeline's footprint indistinguishable from human browsing patterns." |
| 22 | **Dual-Handler Pipeline Logging** | Console-only, CloudWatch/External | Logging to both console (for dev) and a persistent log file (for production traceability) ensures we have a post-mortem trail without needing an external service. | Local disk usage (requires management/rotation) | "I implemented dual-layer logging — real-time visibility in the console and persistent traceability in file logs. This ensures zero data loss for operational tracking." |
| 23 | **Audit-First Ingestion Workflow** | Python-based logs only, No audit | Recording every sync event (record counts, status, errors) in a structured DB table allows for the construction of health dashboards and proactive monitoring. | Extra DB table and write overhead | "I treated observability as a core feature by building a structured auditing layer. Every sync records its 'Pulse' in the database, enabling me to build a health dashboard that proves data reliability before any ML model starts training." |
| 24 | **Structured Audit Summary Contract** | Log-only warnings, ad-hoc SQL checks | Returning machine-readable audit results (`team_stats_violations`, `player_stats_missing_games`, `null_score_matches`, `passed`) makes quality status easy to persist, query, and surface to clients. | Slightly more schema discipline needed in code/tests | "I moved data-quality checks from human-only logs to a formal contract so observability can drive product behavior." |
| 25 | **Additive System Status Exposure** (`pipeline.audit_violations`) | New endpoint, breaking response change | Extending existing `/api/v1/system/status` payload avoids client breakage while adding useful quality telemetry. | Slightly larger response payload | "I shipped quality telemetry as an additive API field so existing consumers stayed stable." |
| 26 | **Rotating File Logs** over plain file handler | Unlimited file logs, external logging stack | Rotating handlers cap log growth and preserve recent history without requiring immediate external logging infrastructure. | Requires tuning rotation size/count | "I added log rotation to keep local ops simple while preventing disk bloat." |
| 27 | **Behavior-Aligned Route Tests** | Force app behavior to old tests, skip failing tests | Tests were updated to reflect actual runtime behavior (`/` serves static dashboard HTML, `/api/v1/health` returns `healthy`). | Requires clear test/docs synchronization | "I treat tests as executable product contracts; when behavior is intentional, tests must assert the real contract." |
| 28 | **Targeted Regression Suite Gate** | Broad but flaky test gate, no targeted gate | Running focused regression set (`ingestion_retry`, `routes`, `config`) gives fast confidence for Phase 0 hardening changes. | Not full end-to-end coverage alone | "I use layered test gates: fast targeted regression first, then deeper integration in follow-up." |
| 29 | **Advanced Metrics Backfill in Incremental Sync** | Full historical reload, One-off migration script, Leave nulls and impute in model | We added selective backfill detection for games missing advanced metrics (`off/def rating`, `pace`, `eFG`, `TS`) and include those game_ids during incremental pulls. This keeps daily sync cheap while repairing legacy incomplete rows. | Slightly more complex filter logic in ingestion | "I designed a targeted backfill strategy: keep watermark-based incremental ingestion, but include historical rows only when critical feature columns are missing. It gives migration quality without full reload cost." |
| 30 | **Two-Stage Defensive Rating Derivation** | Depend only on API-provided defensive metrics, Drop defensive rating feature, Build a separate ETL job | First attempt defensive rating from same-row inputs (`plus_minus` + estimated possessions). Then run a SQL backfill pass using opponent points from self-join when `plus_minus` is unavailable. This maximizes completeness while preserving idempotency. | Derived metric has approximation assumptions tied to possessions formula | "I implemented a fallback hierarchy for feature completeness. If one source field is absent, we derive defensive rating from opponent points and possessions in-DB. This is a production resilience pattern: graceful degradation, not silent nulls." |
| 31 | **Self-Healing Audit Table Bootstrap** | Require manual SQL fix, Reset Docker volume, Hard-fail pipeline on missing table | Legacy Docker volumes can drift from latest schema. We detect missing `pipeline_audit`, auto-create table/indexes idempotently, and retry audit insert once. `/system/status` also degrades gracefully to empty history if table is absent. | Runtime DDL in app code adds bootstrap responsibility to service layer | "I added a self-healing observability bootstrap. Instead of breaking ingestion on schema drift, the system repairs the missing audit table and continues. This reduces operational toil while keeping reliability visible." |
| 32 | **Prediction Operations API Split** (`/predictions/today` + `/predictions/performance`) | Keep only single-game prediction endpoint, Offline notebook-only evaluation | Separating batch serving from performance analytics creates a production-style contract: one route for daily inference, another for historical model governance. | More endpoints to maintain and test | "I separated online inference from evaluation endpoints so runtime serving and model governance can evolve independently." |
| 33 | **Idempotent Prediction Persistence + Outcome Sync** | Recompute predictions on demand only, Overwrite full tables daily | Persisting per-model predictions with `(game_id, model_name)` upsert and syncing `was_correct` post-game creates reproducible accuracy/calibration metrics. | Requires stateful storage lifecycle and backfill logic | "I designed prediction tracking as an auditable ledger, not an ephemeral response. That enables trustworthy backtesting and monitoring." |
| 34 | **Bankroll Ledger APIs over Spreadsheet Tracking** | Manual Excel tracking, Frontend-only local storage | A DB-backed `bets` ledger with create/list/settle/summary endpoints makes bankroll analysis queryable, testable, and automation-ready for risk controls. | Requires settlement rules and API contracts | "I treated bankroll operations as first-class data products. Once bets are in a ledger, ROI and risk metrics become deterministic system outputs, not manual calculations." |
| 35 | **Single-Page Operations Console (Module Layout)** | Multi-page navigation, separate micro-frontends | A single page with module cards/table sections reduces click-friction for daily operator workflows and aligns with interview demos where end-to-end flow must be shown quickly. | Larger page-level script and tighter coupling of module refresh behavior | "I optimized for operational visibility: one screen shows system health, model output, explainability, and bankroll impact." |
| 36 | **Vanilla Frontend Integration over Framework Migration** | React/Next.js rewrite, chart-heavy SPA from day one | At current scope, framework-free integration keeps delivery fast and deployability simple while still proving API-first architecture and UX discipline. | Manual state/render handling is more verbose than component frameworks | "I deferred framework complexity until product needs justified it. This is a pragmatic sequencing decision: prove value and contracts first, then scale the frontend stack." |
| 37 | **Tab-Based Information Architecture** (Home/Raw/Quality/Analysis) | Single long scroll page, multi-route app | Splitting workflows by operator intent makes the dashboard easier to reason about and keeps context-specific controls together (raw exploration vs. monitoring vs. analysis). | Slightly more navigation/state handling in JS | "I used intent-based tabs so each screen answers one question clearly: what data exists, is pipeline healthy, and what do models suggest?" |
| 38 | **Raw-Only Data Explorer Contract** (exclude `match_features`) | Expose all tables, hide DB exploration entirely | Restricting explorer APIs to raw tables keeps experimentation data separate and avoids accidental coupling between exploratory UI and model-feature internals. | Requires dedicated backend endpoints and whitelist maintenance | "I enforced a clean boundary: raw source truth is visible to operators, while engineered features remain controlled for modeling workflows." |
| 39 | **Graceful-Degradation RAG Runtime** (Gemini/Chroma primary, deterministic fallback) | Hard-fail when provider unavailable, mock data, provider lock-in | Local and interview environments are often dependency- or network-constrained. A fallback path preserves endpoint reliability and demonstrates resilient engineering. | Fallback summaries are less expressive than live LLM output | "I designed the intelligence layer to degrade safely: when LLM/vector infra is unavailable, the API still returns deterministic, audit-friendly outputs instead of 500 errors." |
| 40 | **Citation-Gated Intelligence Outputs** | Allow uncited summaries, confidence-only display | Enforcing citation presence reduces hallucination risk and keeps context explainable. This aligns with production governance expectations for LLM usage. | In sparse-context periods, output becomes conservative (`insufficient`) | "I implemented a citation gate: no sources means no authoritative narrative. That protects trust and keeps AI output reviewable." |
| 41 | **Deterministic Rule Overlay separate from model probability** | Blend rules directly into predicted probability, manual analyst override only | Keeping rule signals separate preserves statistical integrity of model outputs while still adding operational context. | Operators must read two signals (probability + context risk) | "I separated model inference from contextual governance. This avoids hidden probability mutations and keeps decision logic auditable." |
| 42 | **API-First MLOps Controls** (`/mlops/monitoring`, `/mlops/retrain/policy`) | Notebook-only monitoring, cron logs without API, manual retrain notes | API contracts make monitoring and retrain decisions portable to dashboards, automation, and alerts. | Additional maintenance surface for endpoints | "I productized MLOps signals as APIs, not notebook artifacts, so operations can consume them consistently across UI and automation layers." |
| 43 | **Model Artifact Snapshot in System Status** | Hidden filesystem state, separate admin-only endpoint | Exposing model artifact metadata in status payload improves operational transparency without extra tooling. | Slightly larger status payload | "I surfaced model artifact metadata in system status so teams can immediately verify which artifacts are active during incident/debug workflows." |

---


*This log is updated as new decisions are made throughout the project.*
